{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (8.1.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipywidgets) (3.0.11)\n",
      "Requirement already satisfied: decorator in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: colorama in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in e:\\6th sem\\nepsense\\venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langdetect\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 21 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   attachments/0/style_list/0  29 non-null     object \n",
      " 1   attachments/0/style_list/1  29 non-null     object \n",
      " 2   attachments/0/style_list/2  8 non-null      object \n",
      " 3   attachments/0/style_list/3  8 non-null      object \n",
      " 4   attachments/0/style_list/4  5 non-null      object \n",
      " 5   commentUrl                  2000 non-null   object \n",
      " 6   commentsCount               124 non-null    float64\n",
      " 7   date                        2000 non-null   object \n",
      " 8   facebookId                  2000 non-null   int64  \n",
      " 9   facebookUrl                 2000 non-null   object \n",
      " 10  feedbackId                  2000 non-null   object \n",
      " 11  id                          2000 non-null   object \n",
      " 12  likesCount                  2000 non-null   int64  \n",
      " 13  postTitle                   2000 non-null   object \n",
      " 14  profileId                   2000 non-null   object \n",
      " 15  profileName                 2000 non-null   object \n",
      " 16  profilePicture              2000 non-null   object \n",
      " 17  profileUrl                  1686 non-null   object \n",
      " 18  text                        1998 non-null   object \n",
      " 19  English Coversion           2000 non-null   object \n",
      " 20  NameDetection               2000 non-null   object \n",
      "dtypes: float64(1), int64(2), object(18)\n",
      "memory usage: 328.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset_df=pd.read_csv(\"../Facebook Datas/dataset_facebook-comments-scraper_2024-05-04_05-39-19-378.csv\")\n",
    "print(dataset_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"preprocessed_text\"]=dataset_df[\"text\"].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
    "# dataset_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file containing the list of emoji unicodes\n",
    "emoji_df = pd.read_csv('Emoji Sheets - Emoji Only.csv')\n",
    "\n",
    "# Extract the emojis into a list\n",
    "emoji_list = emoji_df['Emoji_List'].tolist()\n",
    "\n",
    "# Start the pattern string\n",
    "pattern = '['\n",
    "\n",
    "# Append each code point to the pattern string, ensuring each one is 8 digits\n",
    "for cp in emoji_list:\n",
    "    pattern += f'\\\\U{cp[1:]:0>8}'\n",
    "\n",
    "# Close the pattern string\n",
    "pattern += ']'\n",
    "\n",
    "# Compile the regular expression\n",
    "emoji_pattern = re.compile(pattern, re.UNICODE)\n",
    "\n",
    "# function to completely remove the emojis from the comments using re\n",
    "def remove_emojis(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # or return an empty string: return ''\n",
    "    \n",
    "    # Compile the regular expression\n",
    "    emoji_pattern = re.compile(pattern, re.UNICODE)\n",
    "    \n",
    "    # Use the sub method to remove emojis\n",
    "    text_no_emojis = emoji_pattern.sub(r'', text)\n",
    "    return text_no_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 14020.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "dataset_df[\"preprocessed_text\"]=dataset_df[\"preprocessed_text\"].progress_apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame containing punctuation information\n",
    "data = {\n",
    "    \"Name\": [\"Full stop; Period\", \"Comma\", \"Semicolon\", \"Question Mark\", \"Exclamation Mark\", \"Brackets\", \"Quotation Marks\", \"Dash\", \"Hyphen\"],\n",
    "    \"Nepali Form\": [\"।\", \",\", \";\", \"?\", \"!\", \"(\", \"” “\", \"—\", \"–\"]\n",
    "}\n",
    "punctuation_df = pd.DataFrame(data)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove specified punctuation marks from the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input string from which punctuation will be removed.\n",
    "\n",
    "    Returns:\n",
    "    str: The string with punctuation removed.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text  # or return an empty string: return ''\n",
    "    \n",
    "    for punct in punctuation_df[\"Nepali Form\"]:\n",
    "        text = text.replace(punct, \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 194171.75it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_df[\"preprocessed_text\"] = dataset_df[\"preprocessed_text\"].progress_apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['नेता', 'यत्ति', 'राजनैतिक', 'संस्कार', 'हु']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Nepali_nlp.Nepali_nlp import Stem\n",
    "text = 'नेताहरुमा यत्तिको राजनैतिक संस्कार हुनुपर्छ'\n",
    "Stem().rootify(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nepali_stemmer(text):\n",
    "    \"\"\"\n",
    "    Stem Nepali text using the Stem class.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input string to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "    str: The stemmed string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text  # or return an empty string: return ''\n",
    "    \n",
    "    temp_result = Stem().rootify(text)\n",
    "    temp_result = \" \".join(temp_result)\n",
    "    return temp_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"preprocessed_text\"] = dataset_df[\"preprocessed_text\"].apply(nepali_stemmer)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopword=set(nltk.corpus.stopwords.words('nepali'))\n",
    "stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text, stop_words):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df[\"preprocessed_text\"] = dataset_df[\"preprocessed_text\"].apply(lambda x: remove_stop_words(x,stopword))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
